{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy sentence-transformers tensorflow tensorflow_hub scipy transformers langchain_openai ragas openai load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from numpy.linalg import norm\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer\n",
    "import scipy.special\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragas libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ContextEntityRecall\n",
    "from ragas.metrics import Faithfulness\n",
    "from ragas import SingleTurnSample\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextPrecisionWithReference\n",
    "from ragas.metrics import ContextEntityRecall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Initialize the OpenAI LLM and wrap it\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "wrapped_llm = LangchainLLMWrapper(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def calculate_context_precision(Question, ground_truth_Context, Context):\n",
    "    context_precision = LLMContextPrecisionWithReference(llm=wrapped_llm)\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=Question,\n",
    "        reference=ground_truth_Context,\n",
    "        retrieved_contexts=[Context],\n",
    "    )\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def evaluate_context_entity_recall(ground_truth_Context, Context):\n",
    "#     # Create a sample for evaluation\n",
    "#     sample = SingleTurnSample(\n",
    "#         reference=ground_truth_Context,\n",
    "#         retrieved_contexts=[Context],\n",
    "#     )\n",
    "\n",
    "#     # Initialize the ContextEntityRecall scorer with the wrapped LLM\n",
    "#     scorer = ContextEntityRecall(llm=wrapped_llm)\n",
    "\n",
    "#     # Calculate the score\n",
    "#     score = await scorer.single_turn_ascore(sample)\n",
    "#     print(f\"LLM-based context entities recall with reference answer: {score}\")\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def evaluate_faithfulness(Question, Answer, Context):\n",
    "#     sample = SingleTurnSample(\n",
    "#         user_input=Question,\n",
    "#         response=Answer,\n",
    "#         retrieved_contexts=[Context]\n",
    "#     )\n",
    "\n",
    "#     scorer = Faithfulness(llm=wrapped_llm)\n",
    "#     score = await scorer.single_turn_ascore(sample)\n",
    "#     print(f\"Faithfulness: {score}\")\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticipantVisibleError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return np.dot(a, b) / (norm(a) * norm(b) + 1e-8)  # Avoid division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \n",
    "    # Drop the row ID column\n",
    "    solution = solution.drop(columns=[row_id_column_name])\n",
    "    submission = submission.drop(columns=[row_id_column_name])\n",
    "\n",
    "    # Validate columns\n",
    "    required_submission_cols = {'Question', 'Context', 'Answer', 'Sections', 'Pages'}\n",
    "    required_solution_cols = {'Question', 'ground_truth_Context', 'ground_truth_Answer', 'ground_truth_Sections', 'ground_truth_Pages'}\n",
    "\n",
    "    if not required_submission_cols.issubset(submission.columns):\n",
    "        missing = required_submission_cols - set(submission.columns)\n",
    "        raise ParticipantVisibleError(f\"Missing columns in submission: {missing}\")\n",
    "\n",
    "    if not required_solution_cols.issubset(solution.columns):\n",
    "        missing = required_solution_cols - set(solution.columns)\n",
    "        raise ParticipantVisibleError(f\"Missing columns in solution: {missing}\")\n",
    "\n",
    "    # Merge on 'question'\n",
    "    merged = pd.merge(solution, submission, on='Question', how='inner')\n",
    "\n",
    "    if merged.empty:\n",
    "        raise ParticipantVisibleError(\"No matching questions between submission and solution.\")\n",
    "\n",
    "    # Initialize models\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    bem = hub.load('https://www.kaggle.com/models/google/bert/TensorFlow2/answer-equivalence-bem/1')\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Initialize scores\n",
    "    context_matching_scores = []\n",
    "    context_precision_scores = []\n",
    "    answer_correctness_scores = []\n",
    "    page_corectness_scores = []\n",
    "\n",
    "    for index, row in merged.iterrows():\n",
    "        try:\n",
    "            if not row[\"ground_truth_Answer\"]:\n",
    "                if not row[\"Answer\"]:\n",
    "                    context_matching_scores.append(1)\n",
    "                    context_precision_scores.append(1)\n",
    "                    answer_correctness_scores.append(1)\n",
    "                    page_corectness_scores.append(1)\n",
    "                else:\n",
    "                    context_matching_scores.append(0)\n",
    "                    context_precision_scores.append(0)\n",
    "                    answer_correctness_scores.append(0)\n",
    "                    page_corectness_scores.append(0)\n",
    "                    continue\n",
    "\n",
    "            Context = row[\"Context\"][:200]\n",
    "            q_emb = model.encode(row[\"Question\"])\n",
    "            ctx_emb = model.encode(Context)\n",
    "            sec_emb = model.encode(row[\"Sections\"])\n",
    "            groundsec_emb = model.encode(row[\"ground_truth_Sections\"])\n",
    "            context_matching_scores.append(cosine_sim(q_emb, ctx_emb) + cosine_sim(sec_emb, groundsec_emb))\n",
    "\n",
    "            Question = row[\"Question\"]\n",
    "            ground_truth_Context = row[\"ground_truth_Context\"]\n",
    "            ground_truth_Answer = row[\"ground_truth_Answer\"]\n",
    "            Answer = row[\"Answer\"]\n",
    "\n",
    "            precision_score = await calculate_context_precision(Question, ground_truth_Context, Context)            \n",
    "            context_precision_scores.append(precision_score)\n",
    "\n",
    "            input_text = f\"[CLS] {Question} [SEP] {ground_truth_Answer} [SEP] {Answer} [SEP]\"\n",
    "            # Tokenize input\n",
    "            encoded = tokenizer(\n",
    "                input_text,\n",
    "                return_tensors=\"tf\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "            )\n",
    "            # Prepare input dict with correct types\n",
    "            inputs = {\n",
    "                \"input_ids\": tf.cast(encoded[\"input_ids\"], tf.int64),\n",
    "                \"segment_ids\": tf.cast(encoded[\"token_type_ids\"], tf.int64)\n",
    "            }\n",
    "            # Run model\n",
    "            raw_outputs = bem(inputs)\n",
    "            # Convert logits to probabilities using softmax\n",
    "            probabilities = scipy.special.softmax(raw_outputs.numpy(), axis=1)\n",
    "            # BERT Answer Equivalence Score (Probability of class 1)\n",
    "            equivalence_score = probabilities[0][1]\n",
    "            answer_correctness_scores.append(equivalence_score)\n",
    "\n",
    "            # Extract ground truth pages and submission pages\n",
    "            ground_truth_pages = set(row[\"ground_truth_Pages\"].split(',')) if isinstance(row[\"ground_truth_Pages\"], str) else {row[\"ground_truth_Pages\"]}\n",
    "            submission_pages = set(row[\"Pages\"].split(',')) if isinstance(row[\"Pages\"], str) else {row[\"Pages\"]}\n",
    "            # Calculate the number of correctly given pages\n",
    "            correct_pages = ground_truth_pages.intersection(submission_pages)\n",
    "            total_pages = len(ground_truth_pages)\n",
    "            # Calculate the proportion of correctly given pages\n",
    "            if total_pages > 0:\n",
    "                page_proportion = len(correct_pages) / total_pages\n",
    "            else:\n",
    "                page_proportion = 0\n",
    "            # Append the proportion to the scores list\n",
    "            page_corectness_scores.append(page_proportion)\n",
    "            print(f\"Row {index} evaluated\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ParticipantVisibleError(f\"Embedding computation failed for a row: {e}\")\n",
    "\n",
    "    # Convert to numpy arrays and clip values\n",
    "    context_matching_scores = np.clip(np.array(context_matching_scores), 0, 1)\n",
    "    context_precision_scores = np.clip(np.array(context_precision_scores), 0, 1)\n",
    "    answer_correctness_scores = np.clip(np.array(answer_correctness_scores), 0, 1)\n",
    "    page_corectness_scores = np.clip(np.array(page_corectness_scores), 0, 1)  # Corrected this line\n",
    "    \n",
    "    # Compute weighted average\n",
    "    final_scores = (\n",
    "        0.2 * context_matching_scores +\n",
    "        0.3 * context_precision_scores +\n",
    "        0.4 * answer_correctness_scores +\n",
    "        0.1 * page_corectness_scores\n",
    "    )\n",
    "\n",
    "    # Add total score to submission DataFrame\n",
    "    Total_Score = final_scores.mean() * 90\n",
    "    print(f\"Total score is {Total_Score} out of 90\")\n",
    "\n",
    "    return context_matching_scores, context_precision_scores, answer_correctness_scores, page_corectness_scores, final_scores, Total_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query ID</th>\n",
       "      <th>Question</th>\n",
       "      <th>ground_truth_Context</th>\n",
       "      <th>ground_truth_Answer</th>\n",
       "      <th>ground_truth_Sections</th>\n",
       "      <th>ground_truth_Pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Why did the English begin to focus more on Sri...</td>\n",
       "      <td>The English began to pay more attention to Sri...</td>\n",
       "      <td>The English began to focus on Sri Lanka due to...</td>\n",
       "      <td>2.2 The British focusing their Attention on Sr...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>What administrative practices did the British ...</td>\n",
       "      <td>When the English East India Trade Company gain...</td>\n",
       "      <td>The British East India Trade Company governed ...</td>\n",
       "      <td>Governance of the Coastal Areas under the East...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>What was the significance of the establishment...</td>\n",
       "      <td>A landmark in Buddhist education field was the...</td>\n",
       "      <td>The Parama Dhamma Chethiya Pirivena, founded b...</td>\n",
       "      <td>3.2 Buddhist Renaissance</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>What were the major contributions of Arumuga N...</td>\n",
       "      <td>There was a religious and a cultural renaissan...</td>\n",
       "      <td>Arumuga Navalar was a key leader of the Hindu ...</td>\n",
       "      <td>3.3 Hindu Religious Renaissance</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>What were the main reasons that led the upcoun...</td>\n",
       "      <td>Although the people of the upcountry could esc...</td>\n",
       "      <td>The people of the upcountry rose against the B...</td>\n",
       "      <td>2.4. Protests against Foreign Domination</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Query ID                                           Question  \\\n",
       "0         6  Why did the English begin to focus more on Sri...   \n",
       "1         7  What administrative practices did the British ...   \n",
       "2         8  What was the significance of the establishment...   \n",
       "3         9  What were the major contributions of Arumuga N...   \n",
       "4        10  What were the main reasons that led the upcoun...   \n",
       "\n",
       "                                ground_truth_Context  \\\n",
       "0  The English began to pay more attention to Sri...   \n",
       "1  When the English East India Trade Company gain...   \n",
       "2  A landmark in Buddhist education field was the...   \n",
       "3  There was a religious and a cultural renaissan...   \n",
       "4  Although the people of the upcountry could esc...   \n",
       "\n",
       "                                 ground_truth_Answer  \\\n",
       "0  The English began to focus on Sri Lanka due to...   \n",
       "1  The British East India Trade Company governed ...   \n",
       "2  The Parama Dhamma Chethiya Pirivena, founded b...   \n",
       "3  Arumuga Navalar was a key leader of the Hindu ...   \n",
       "4  The people of the upcountry rose against the B...   \n",
       "\n",
       "                               ground_truth_Sections  ground_truth_Pages  \n",
       "0  2.2 The British focusing their Attention on Sr...                  20  \n",
       "1  Governance of the Coastal Areas under the East...                  24  \n",
       "2                           3.2 Buddhist Renaissance                  43  \n",
       "3                    3.3 Hindu Religious Renaissance                  48  \n",
       "4           2.4. Protests against Foreign Domination                  32  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution = pd.read_csv(\"Solution.csv\")\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:You try to use a model that was created with version 3.4.1, however, your version is 3.1.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating file: submission_1.csv\n",
      "Row 0 evaluated\n",
      "Row 1 evaluated\n",
      "Row 2 evaluated\n",
      "Row 3 evaluated\n",
      "Row 4 evaluated\n",
      "Total score is 84.41650503603825 out of 90\n"
     ]
    }
   ],
   "source": [
    "submissions_folder = \"submissions\"\n",
    "for filename in os.listdir(submissions_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        submission_path = os.path.join(submissions_folder, filename)\n",
    "        submission = pd.read_csv(submission_path)\n",
    "        print(f\"Evaluating file: {filename}\")\n",
    "        context_matching_scores,context_precision_scores,answer_correctness_scores,page_corectness_scores,final_scores,Total_Score  = await score(solution, submission, \"Query ID\")\n",
    "        submission['Context Matching Score'] = context_matching_scores\n",
    "        submission['Context Precision Score'] = context_precision_scores\n",
    "        submission['Answer Correctness Score'] = answer_correctness_scores\n",
    "        submission['Page Corectness Score'] = page_corectness_scores\n",
    "        submission['Final Score'] = final_scores\n",
    "        submission.loc[0, 'Total Score'] = Total_Score\n",
    "        submission.to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
