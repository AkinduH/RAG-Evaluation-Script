{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy sentence-transformers tensorflow tensorflow_hub scipy transformers langchain_openai ragas openai load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from numpy.linalg import norm\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer\n",
    "import scipy.special\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragas libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ContextEntityRecall\n",
    "from ragas.metrics import Faithfulness\n",
    "from ragas import SingleTurnSample\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextPrecisionWithReference\n",
    "from ragas.metrics import ContextEntityRecall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Initialize the OpenAI LLM and wrap it\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "wrapped_llm = LangchainLLMWrapper(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def calculate_context_precision(Question, ground_truth_Context, Context):\n",
    "    context_precision = LLMContextPrecisionWithReference(llm=wrapped_llm)\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=Question,\n",
    "        reference=ground_truth_Context,\n",
    "        retrieved_contexts=[Context],\n",
    "    )\n",
    "    score = await context_precision.single_turn_ascore(sample)\n",
    "    print(f\"LLM-based context precision with reference: {score}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_context_entity_recall(ground_truth_Context, Context):\n",
    "    # Create a sample for evaluation\n",
    "    sample = SingleTurnSample(\n",
    "        reference=ground_truth_Context,\n",
    "        retrieved_contexts=[Context],\n",
    "    )\n",
    "\n",
    "    # Initialize the ContextEntityRecall scorer with the wrapped LLM\n",
    "    scorer = ContextEntityRecall(llm=wrapped_llm)\n",
    "\n",
    "    # Calculate the score\n",
    "    score = await scorer.single_turn_ascore(sample)\n",
    "    print(f\"LLM-based context entities recall with reference answer: {score}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_faithfulness(Question, Answer, Context):\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=Question,\n",
    "        response=Answer,\n",
    "        retrieved_contexts=[Context]\n",
    "    )\n",
    "\n",
    "    scorer = Faithfulness(llm=wrapped_llm)\n",
    "    score = await scorer.single_turn_ascore(sample)\n",
    "    print(f\"Faithfulness: {score}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticipantVisibleError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return np.dot(a, b) / (norm(a) * norm(b) + 1e-8)  # Avoid division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \n",
    "    # Drop the row ID column\n",
    "    solution = solution.drop(columns=[row_id_column_name])\n",
    "    submission = submission.drop(columns=[row_id_column_name])\n",
    "\n",
    "    # Validate columns\n",
    "    required_submission_cols = {'Question', 'Context', 'Answer', 'Sections', 'Pages'}\n",
    "    required_solution_cols = {'Question', 'ground_truth_Context', 'ground_truth_Answer', 'ground_truth_Sections', 'ground_truth_Pages'}\n",
    "\n",
    "    if not required_submission_cols.issubset(submission.columns):\n",
    "        missing = required_submission_cols - set(submission.columns)\n",
    "        raise ParticipantVisibleError(f\"Missing columns in submission: {missing}\")\n",
    "\n",
    "    if not required_solution_cols.issubset(solution.columns):\n",
    "        missing = required_solution_cols - set(solution.columns)\n",
    "        raise ParticipantVisibleError(f\"Missing columns in solution: {missing}\")\n",
    "\n",
    "    # Merge on 'question'\n",
    "    merged = pd.merge(solution, submission, on='Question', how='inner')\n",
    "\n",
    "    if merged.empty:\n",
    "        raise ParticipantVisibleError(\"No matching questions between submission and solution.\")\n",
    "\n",
    "    # Initialize models\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    bem = hub.load('https://www.kaggle.com/models/google/bert/TensorFlow2/answer-equivalence-bem/1')\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Initialize scores\n",
    "    context_matching_scores = []\n",
    "    context_precision_scores = []\n",
    "    context_entity_recall_scores = []\n",
    "    faithfulness_scores = []\n",
    "    answer_correctness_scores = []\n",
    "\n",
    "    for _, row in merged.iterrows():\n",
    "        try:\n",
    "            q_emb = model.encode(row[\"Question\"])\n",
    "            ctx_emb = model.encode(row[\"Context\"])\n",
    "            context_matching_scores.append(cosine_sim(q_emb, ctx_emb))\n",
    "\n",
    "            Question = row[\"Question\"]\n",
    "            Context = row[\"Context\"]\n",
    "            ground_truth_Context = row[\"ground_truth_Context\"]\n",
    "            ground_truth_Answer = row[\"ground_truth_Answer\"]\n",
    "            Answer = row[\"Answer\"]\n",
    "\n",
    "            precision_score = await calculate_context_precision(Question, ground_truth_Context, Context)\n",
    "            entity_recall_score = await evaluate_context_entity_recall(ground_truth_Context, Context)\n",
    "            faithfulness_score = await evaluate_faithfulness(Question, Answer, Context)\n",
    "            \n",
    "            context_precision_scores.append(precision_score)\n",
    "            context_entity_recall_scores.append(entity_recall_score)\n",
    "            faithfulness_scores.append(faithfulness_score)\n",
    "\n",
    "            input_text = f\"[CLS] {Question} [SEP] {ground_truth_Answer} [SEP] {Answer} [SEP]\"\n",
    "            # Tokenize input\n",
    "            encoded = tokenizer(\n",
    "                input_text,\n",
    "                return_tensors=\"tf\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "            )\n",
    "            # Prepare input dict with correct types\n",
    "            inputs = {\n",
    "                \"input_ids\": tf.cast(encoded[\"input_ids\"], tf.int64),\n",
    "                \"segment_ids\": tf.cast(encoded[\"token_type_ids\"], tf.int64)\n",
    "            }\n",
    "            # Run model\n",
    "            raw_outputs = bem(inputs)\n",
    "            # Convert logits to probabilities using softmax\n",
    "            probabilities = scipy.special.softmax(raw_outputs.numpy(), axis=1)\n",
    "            # BERT Answer Equivalence Score (Probability of class 1)\n",
    "            equivalence_score = probabilities[0][1]\n",
    "            answer_correctness_scores.append(equivalence_score)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ParticipantVisibleError(f\"Embedding computation failed for a row: {e}\")\n",
    "\n",
    "    # Convert to numpy arrays and clip values\n",
    "    context_matching_scores = np.clip(np.array(context_matching_scores), 0, 1)\n",
    "    context_precision_scores = np.clip(np.array(context_precision_scores), 0, 1)\n",
    "    context_entity_recall_scores = np.clip(np.array(context_entity_recall_scores), 0, 1)\n",
    "    faithfulness_scores = np.clip(np.array(faithfulness_scores), 0, 1)\n",
    "    answer_correctness_scores = np.clip(np.array(answer_correctness_scores), 0, 1)\n",
    "    \n",
    "    # Compute weighted average\n",
    "    final_scores = (\n",
    "        0.1 * context_matching_scores +\n",
    "        0.2 * context_precision_scores +\n",
    "        0.2 * context_entity_recall_scores +\n",
    "        0.2 * faithfulness_scores +\n",
    "        0.3 * answer_correctness_scores\n",
    "    )\n",
    "\n",
    "    # Add total score to submission DataFrame\n",
    "    Total_Score = final_scores.mean() * 90\n",
    "    print(f\"Total score is {Total_Score} out of 90\")\n",
    "\n",
    "    return context_matching_scores,context_precision_scores,context_entity_recall_scores,faithfulness_scores,answer_correctness_scores,final_scores,Total_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query ID</th>\n",
       "      <th>Question</th>\n",
       "      <th>ground_truth_Context</th>\n",
       "      <th>ground_truth_Answer</th>\n",
       "      <th>ground_truth_Sections</th>\n",
       "      <th>ground_truth_Pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Why did the English begin to focus more on Sri...</td>\n",
       "      <td>The English began to pay more attention to Sri...</td>\n",
       "      <td>The English began to focus on Sri Lanka due to...</td>\n",
       "      <td>2.2 The British focusing their Attention on Sr...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>What administrative practices did the British ...</td>\n",
       "      <td>When the English East India Trade Company gain...</td>\n",
       "      <td>The British East India Trade Company governed ...</td>\n",
       "      <td>Governance of the Coastal Areas under the East...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Query ID                                           Question  \\\n",
       "0         6  Why did the English begin to focus more on Sri...   \n",
       "1         7  What administrative practices did the British ...   \n",
       "\n",
       "                                ground_truth_Context  \\\n",
       "0  The English began to pay more attention to Sri...   \n",
       "1  When the English East India Trade Company gain...   \n",
       "\n",
       "                                 ground_truth_Answer  \\\n",
       "0  The English began to focus on Sri Lanka due to...   \n",
       "1  The British East India Trade Company governed ...   \n",
       "\n",
       "                               ground_truth_Sections  ground_truth_Pages  \n",
       "0  2.2 The British focusing their Attention on Sr...                  20  \n",
       "1  Governance of the Coastal Areas under the East...                  24  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution = pd.read_csv(\"Solution.csv\")\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:You try to use a model that was created with version 3.4.1, however, your version is 3.1.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 1\n",
      "LLM-based context precision with reference: 0.9999999999\n",
      "LLM-based context entities recall with reference answer: 0.9999999985714286\n",
      "Faithfulness: 1.0\n",
      "done 4\n",
      "done 5\n",
      "done 1\n",
      "LLM-based context precision with reference: 0.9999999999\n",
      "LLM-based context entities recall with reference answer: 0.9999999992857143\n",
      "Faithfulness: 0.7\n",
      "done 4\n",
      "done 5\n",
      "Total score is 78.92337913358595 out of 90\n"
     ]
    }
   ],
   "source": [
    "submissions_folder = \"submissions\"\n",
    "for filename in os.listdir(submissions_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        submission_path = os.path.join(submissions_folder, filename)\n",
    "        submission = pd.read_csv(submission_path)\n",
    "        context_matching_scores,context_precision_scores,context_entity_recall_scores,faithfulness_scores,answer_correctness_scores,final_scores,Total_Score  = await score(solution, submission, \"Query ID\")\n",
    "        submission['Context Matching Score'] = context_matching_scores\n",
    "        submission['Context Precision Score'] = context_precision_scores\n",
    "        submission['Context Entity Recall Score'] = context_entity_recall_scores\n",
    "        submission['Faithfulness Score'] = faithfulness_scores\n",
    "        submission['Answer Correctness Score'] = answer_correctness_scores\n",
    "        submission['Final Score'] = final_scores\n",
    "        submission.loc[0, 'Total Score'] = Total_Score\n",
    "        submission.to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
